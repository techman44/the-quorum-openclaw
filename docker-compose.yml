version: '3.8'

services:
  # ─── PostgreSQL + pgvector ───────────────────────────────────────────
  # Stores conversation history, embeddings, and plugin state.
  # Schema migrations are applied automatically on first run via
  # the files mounted into /docker-entrypoint-initdb.d.
  postgres:
    image: pgvector/pgvector:pg17
    container_name: quorum-db
    environment:
      POSTGRES_USER: ${DB_USER:-quorum}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-changeme}
      POSTGRES_DB: ${DB_NAME:-quorum}
    ports:
      - "${DB_PORT:-5432}:5432"
    volumes:
      - quorum_data:/var/lib/postgresql/data
      - ./schema:/docker-entrypoint-initdb.d
    restart: unless-stopped

  # ─── Ollama ──────────────────────────────────────────────────────────
  # Local LLM inference server. Pull models after startup with:
  #   docker exec quorum-ollama ollama pull <model>
  ollama:
    image: ollama/ollama:latest
    container_name: quorum-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # ── GPU support (optional) ──────────────────────────────────────────
    # Uncomment the block below if you have an NVIDIA GPU and the
    # NVIDIA Container Toolkit installed. This enables GPU-accelerated
    # embeddings (~60-90ms vs ~500ms+ on CPU). See README for details.
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  quorum_data:
  ollama_data:
